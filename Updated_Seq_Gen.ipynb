{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Config, T5ForConditionalGeneration, PreTrainedTokenizerBase, AdamW, AddedToken\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from typing import Dict\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PgDWN5iHVOgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/seq2seq_data1.xlsx\""
      ],
      "metadata": {
        "id": "znpS3SMtsg5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_excel(file_path, separator='|'):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'EVENTS_SEQ' not in df.columns:\n",
        "        raise ValueError(\"Excel file must contain 'EVENTS_SEQ' column.\")\n",
        "    df.reset_index(drop=False, inplace=True)\n",
        "    df.rename(columns={'index': 'User ID', 'EVENTS_SEQ': 'Sequence of events'}, inplace=True)\n",
        "    df['Sequence of events'] = df['Sequence of events'].astype(str)\n",
        "    df['Sequence of events'] = df['Sequence of events'].apply(lambda x: x.split(separator))\n",
        "    df = df[df['Sequence of events'].apply(len) > 10]\n",
        "    df = df[['User ID', 'Sequence of events']]\n",
        "    return df"
      ],
      "metadata": {
        "id": "7UCHtpeZw6NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = read_data_from_excel(input_file, separator='|')\n",
        "print(dataframe.head(10))\n",
        "print(len(dataframe))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni-w18UMw9t9",
        "outputId": "338de068-c13f-4f32-f2e0-75b1f119c641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     User ID                                 Sequence of events\n",
            "1          1  [first_visit, session_start, open_chat, open_c...\n",
            "2          2  [first_visit, session_start, open_chat, open_c...\n",
            "3          3  [first_visit, session_start, open_chat, open_c...\n",
            "4          4  [first_visit, session_start, open_chat, open_c...\n",
            "7          7  [first_visit, session_start, open_chat, close_...\n",
            "9          9  [first_visit, session_start, open_chat, page_v...\n",
            "18        18  [first_visit, session_start, open_chat, view_i...\n",
            "446      446  [first_visit, session_start, page_view, view_i...\n",
            "454      454  [first_visit, session_start, at_visibility, ge...\n",
            "467      467  [first_visit, session_start, at_visibility, pa...\n",
            "2233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD52cf1hZi1I",
        "outputId": "c8adff70-7f38-42bf-bc13-c4c0af75c670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Sequence of events'][4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlakQ5LCa89F",
        "outputId": "f42ccd51-4494-4aec-c945-73f28a4fbecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['first_visit', 'session_start', 'open_chat', 'open_chat', 'open_chat', 'open_chat', 'open_chat', 'page_view', 'page_view', 'proactive_message_impression', 'proactive_message_impression']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalDictionary:\n",
        "    def __init__(self):\n",
        "        self.event_to_index = {\n",
        "            \"START\": 0,\n",
        "            \"END\": 1,\n",
        "            \"PAD\": 2,\n",
        "            \"UNK\": 3,\n",
        "        }\n",
        "        self.index_to_event = {\n",
        "            0: \"START\",\n",
        "            1: \"END\",\n",
        "            2: \"PAD\",\n",
        "            3: \"UNK\",\n",
        "        }\n",
        "        self.counter = 4\n",
        "        self.fixed_vocab = False\n",
        "    def update_dictionary(self, sequence):\n",
        "        for event in sequence:\n",
        "            if not self.fixed_vocab and event not in self.event_to_index:\n",
        "                self.event_to_index[event] = self.counter\n",
        "                self.index_to_event[self.counter] = event\n",
        "                self.counter += 1\n",
        "    def convert_sequence_to_indices(self, sequence):\n",
        "        indices = [self.event_to_index.get(event, self.event_to_index[\"UNK\"]) for event in sequence]\n",
        "        return indices\n",
        "    def fix_vocab(self):\n",
        "        self.fixed_vocab = True"
      ],
      "metadata": {
        "id": "wBqZExnqa-e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EventSequenceProcessor:\n",
        "    def __init__(self, separator='|'):\n",
        "        self.global_dict = GlobalDictionary()\n",
        "        self.dataframe = pd.DataFrame(columns=[\"User ID\", \"Sequence of events\"])\n",
        "        self.separator = separator\n",
        "        self.max_length = 0\n",
        "    def add_data(self, new_data):\n",
        "        new_df = pd.DataFrame(new_data)\n",
        "        for sequence in new_df['Sequence of events']:\n",
        "            self.global_dict.update_dictionary(sequence)\n",
        "        new_df['Sequence of events'] = new_df['Sequence of events'].apply(lambda x: self.global_dict.convert_sequence_to_indices(x))\n",
        "        self.dataframe = pd.concat([self.dataframe, new_df], ignore_index=True)\n",
        "        self.max_length = max(self.max_length, max(new_df['Sequence of events'].apply(len)))\n",
        "    def fix_vocabulary(self):\n",
        "        self.global_dict.fix_vocab()\n",
        "    def get_dataloader(self, batch_size=16):\n",
        "        dataset = EventSequenceDataset(self.dataframe, self.global_dict)\n",
        "        vocab_size = len(self.global_dict.event_to_index)\n",
        "        max_length = self.max_length\n",
        "        def collate_fn(batch):\n",
        "            sequences, targets = zip(*batch)\n",
        "            # sequences = [list(map(int, seq)) for seq in sequences]\n",
        "            # targets = [list(map(int, tgt)) for tgt in targets]\n",
        "            # padded_sequences = nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in sequences], batch_first=True, padding_value=self.global_dict.event_to_index[\"PAD\"])\n",
        "            # padded_targets = nn.utils.rnn.pad_sequence([torch.tensor(tgt) for tgt in targets], batch_first=True, padding_value=self.global_dict.event_to_index[\"PAD\"])\n",
        "            sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
        "            targets = [torch.tensor(tgt, dtype=torch.long) for tgt in targets]\n",
        "            padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.global_dict.event_to_index[\"PAD\"])\n",
        "            padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=self.global_dict.event_to_index[\"PAD\"])\n",
        "            return padded_sequences, padded_targets\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn), vocab_size, max_length\n",
        "    def get_dictionary(self):\n",
        "        return self.global_dict.event_to_index\n",
        "    def get_index_to_event_mapping(self):\n",
        "        return self.global_dict.index_to_event"
      ],
      "metadata": {
        "id": "UhMLAQnEbAtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EventSequenceDataset(Dataset):\n",
        "    def __init__(self, dataframe, global_dict):\n",
        "        self.dataframe = dataframe\n",
        "        self.global_dict = global_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.dataframe.iloc[idx, 1]\n",
        "        input_ids = torch.tensor(self.global_dict.convert_sequence_to_indices(sequence[:-1]), dtype=torch.long)\n",
        "        target_ids = torch.tensor(self.global_dict.convert_sequence_to_indices(sequence[1:]), dtype=torch.long)\n",
        "        return input_ids, target_ids"
      ],
      "metadata": {
        "id": "ozMQ21KVbC9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer(PreTrainedTokenizerBase):\n",
        "    def __init__(self, global_dict):\n",
        "        super().__init__()\n",
        "        self.global_dict = global_dict\n",
        "    def _tokenize(self, text):\n",
        "        tokens = text.split(' ')\n",
        "        return tokens\n",
        "    def _convert_token_to_id(self, token):\n",
        "        return self.global_dict.event_to_index.get(token, self.global_dict.event_to_index[\"UNK\"])\n",
        "    def _convert_id_to_token(self, index):\n",
        "        return self.global_dict.index_to_event.get(index, \"UNK\")\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return ' '.join(tokens)\n",
        "    def save_vocabulary(self, save_directory, filename_prefix=None):\n",
        "        vocab_file = os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + 'vocab.json')\n",
        "        with open(vocab_file, 'w') as f:\n",
        "            json.dump(self.global_dict.event_to_index, f, indent=2)\n",
        "        return (vocab_file,)\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        return token_ids_0\n",
        "    def get_vocab(self):\n",
        "        return self.global_dict.event_to_index\n",
        "    @property\n",
        "    def added_tokens_decoder(self):\n",
        "        return {index: AddedToken(token, lstrip=False, rstrip=False) for token, index in self.get_vocab().items()}\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self._convert_token_to_id(token) for token in tokens]"
      ],
      "metadata": {
        "id": "iZxOqVXUbFoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomT5Model(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=512, num_layers=4, num_heads=4):\n",
        "        super(CustomT5Model, self).__init__()\n",
        "        config = T5Config(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            d_ff=hidden_size * 4,\n",
        "            feed_forward_proj='relu',\n",
        "            is_encoder_decoder=True,\n",
        "            decoder_start_token_id=0,\n",
        "            eos_token_id=1,\n",
        "            # bos_token_id=0,\n",
        "            pad_token_id=2\n",
        "        )\n",
        "        self.model = T5ForConditionalGeneration(config).to(device)\n",
        "        self.model.config.decoder_start_token_id = config.pad_token_id\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        input_ids = input_ids.long()\n",
        "        if labels is not None:\n",
        "            labels = labels.long()\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return outputs.loss, outputs.logits\n",
        "    def save_pretrained(self, save_directory):\n",
        "        self.model.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "-OdXJS9XbHgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def train_and_validate_model(model, train_dataloader, val_dataloader, vocab_size, index_to_event, epochs=8, device=None, teacher_forcing_ratio=0.5):\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    scaler = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=2)\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss_total = 0\n",
        "        train_steps = 0\n",
        "        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", leave=False)\n",
        "        for sequences, targets in train_progress_bar:\n",
        "            sequences, targets = sequences.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                teacher_force = random.random() < teacher_forcing_ratio\n",
        "                if teacher_force:\n",
        "                    teacher_forcing_input = targets[:, :-1].clone().detach()\n",
        "                    teacher_forcing_input = torch.cat([torch.full((sequences.size(0), 1), vocab_size, dtype=torch.long).to(device), teacher_forcing_input], dim=1)\n",
        "                    decoder_input = teacher_forcing_input\n",
        "                else:\n",
        "                    decoder_input = sequences\n",
        "                loss, logits = model(input_ids=sequences, labels=targets)\n",
        "                active_loss = targets.view(-1) != 2\n",
        "                logits = logits[:, :targets.size(1), :]\n",
        "                logits = logits.view(-1, logits.size(-1))\n",
        "                targets = targets.view(-1)\n",
        "                active_loss = active_loss[:logits.size(0)]\n",
        "                active_logits = logits[active_loss]\n",
        "                active_labels = targets[active_loss]\n",
        "                loss = criterion(active_logits, active_labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            train_loss_total += loss.item()\n",
        "            train_steps += 1\n",
        "            train_progress_bar.set_postfix(loss=loss.item())\n",
        "        avg_train_loss = train_loss_total / train_steps\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss_total = 0\n",
        "        val_steps = 0\n",
        "        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for sequences, targets in val_progress_bar:\n",
        "                sequences, targets = sequences.to(device), targets.to(device)\n",
        "                with autocast():\n",
        "                    loss, logits = model(input_ids=sequences, labels=targets)\n",
        "                    active_loss = targets.view(-1) != 2\n",
        "                    logits = logits[:, :targets.size(1), :]\n",
        "                    logits = logits.view(-1, logits.size(-1))\n",
        "                    targets = targets.view(-1)\n",
        "                    active_loss = active_loss[:logits.size(0)]\n",
        "                    active_logits = logits[active_loss]\n",
        "                    active_labels = targets[active_loss]\n",
        "                    val_loss = criterion(active_logits, active_labels)\n",
        "                val_loss_total += val_loss.item()\n",
        "                val_steps += 1\n",
        "                val_progress_bar.set_postfix(loss=val_loss.item())\n",
        "        avg_val_loss = val_loss_total / val_steps\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Validation Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "pO3UKYn5bJPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = EventSequenceProcessor()\n",
        "processor.add_data(dataframe)\n",
        "processor.fix_vocabulary()"
      ],
      "metadata": {
        "id": "pe1qBVjbbNUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(processor.dataframe, test_size=0.2, random_state=42)\n",
        "train_processor = EventSequenceProcessor()\n",
        "val_processor = EventSequenceProcessor()\n",
        "train_processor.add_data(train_df)\n",
        "val_processor.add_data(val_df)\n",
        "train_dataloader, _, _ = train_processor.get_dataloader()\n",
        "val_dataloader, _, _ = val_processor.get_dataloader()"
      ],
      "metadata": {
        "id": "5DlFUibCbP_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tokenizer = CustomTokenizer(processor.global_dict)\n",
        "vocab_size = len(custom_tokenizer.get_vocab())\n",
        "index_to_event = processor.get_index_to_event_mapping()"
      ],
      "metadata": {
        "id": "1eIlbGhZbRs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomT5Model(vocab_size)\n",
        "train_and_validate_model(model, train_dataloader, val_dataloader, vocab_size, index_to_event, epochs=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4e4Mip_bTrq",
        "outputId": "de3d0f49-7c61-4b3e-bad9-041dea6bd32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/8 - Training:   0%|          | 0/112 [00:00<?, ?it/s]<ipython-input-26-2d9e8a3e54e6>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
            "<ipython-input-26-2d9e8a3e54e6>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = [torch.tensor(tgt, dtype=torch.long) for tgt in targets]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8, Average Training Loss: 2.5963\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8, Average Validation Loss: 5.2365\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/8, Average Training Loss: 1.8341\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/8, Average Validation Loss: 5.0308\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/8, Average Training Loss: 1.6673\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/8, Average Validation Loss: 5.1338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/8, Average Training Loss: 1.5693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/8, Average Validation Loss: 4.9291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/8, Average Training Loss: 1.5289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/8, Average Validation Loss: 4.9388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/8, Average Training Loss: 1.4562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/8, Average Validation Loss: 5.0033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/8, Average Training Loss: 1.4122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/8, Average Validation Loss: 5.1679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/8, Average Training Loss: 1.3785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/8, Average Validation Loss: 4.9716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/custom_t5_model\"\n",
        "model.save_pretrained(model_path)\n",
        "custom_tokenizer.save_vocabulary(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2e9ocK9bWAZ",
        "outputId": "cade8d07-01d3-484d-8d26-3e98478e8337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/custom_t5_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model, tokenizer, input_sequence, prediction_steps=3, num_beams=5):\n",
        "    input_ids = torch.tensor([tokenizer.global_dict.convert_sequence_to_indices(input_sequence)]).to(device)\n",
        "    model.eval()\n",
        "    generated_sequence = input_sequence.copy()\n",
        "\n",
        "    for _ in range(prediction_steps):\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_length=input_ids.size(1) + 1,\n",
        "                num_beams=num_beams,\n",
        "                pad_token_id=tokenizer.global_dict.event_to_index[\"PAD\"],\n",
        "                decoder_start_token_id=tokenizer.global_dict.event_to_index[\"START\"],\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Get the next predicted token (excluding the input sequence)\n",
        "        next_token_id = output_ids[0, input_ids.size(1):].item()\n",
        "        next_token = tokenizer._convert_id_to_token(next_token_id)\n",
        "\n",
        "        # Append the predicted token to the generated sequence\n",
        "        generated_sequence.append(next_token)\n",
        "\n",
        "        # Update the input_ids for the next prediction\n",
        "        input_ids = torch.tensor([tokenizer.global_dict.convert_sequence_to_indices(generated_sequence)]).to(device)\n",
        "\n",
        "    return generated_sequence\n"
      ],
      "metadata": {
        "id": "YW7Ytw2N2j40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sequence = df['Sequence of events'][454][:8]  # Original sequence of length n-3\n",
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Original Sequence:\",df['Sequence of events'][454])\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, prediction_steps=3, num_beams=5)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-47HqPJw2m3i",
        "outputId": "eb03f52c-e5cd-420e-fcad-e7f455b68948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: ['first_visit', 'session_start', 'at_visibility', 'general_tab_click', 'general_tab_click', 'open_chat', 'general_tab_click', 'general_tab_click']\n",
            "Original Sequence: ['first_visit', 'session_start', 'at_visibility', 'general_tab_click', 'general_tab_click', 'open_chat', 'general_tab_click', 'general_tab_click', 'general_tab_click', 'page_view', 'user_engagement']\n",
            "Generated Sequence: ['first_visit', 'session_start', 'at_visibility', 'general_tab_click', 'general_tab_click', 'open_chat', 'general_tab_click', 'general_tab_click', 'open_chat', 'open_chat', 'open_chat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sequence = df['Sequence of events'][467][:8]  # Original sequence of length n-3\n",
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Original Sequence:\",df['Sequence of events'][467])\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, prediction_steps=3, num_beams=5)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj3RUzdnNdYD",
        "outputId": "cbf28963-1070-42ec-c4f6-c787ce5a8c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: ['first_visit', 'session_start', 'at_visibility', 'page_view', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click']\n",
            "Original Sequence: ['first_visit', 'session_start', 'at_visibility', 'page_view', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'user_engagement', 'page_view', 'banner_button_click', 'banner_button_click', 'user_engagement', 'page_view', 'at_visibility', 'general_tab_click', 'fnb_menu_link_click', 'user_engagement', 'page_view', 'general_link_click', 'page_view', 'view_item_list', 'select_item', 'user_engagement', 'product_learn_more', 'page_view', 'session_start', 'user_engagement', 'session_start', 'page_view', 'general_button_click', 'gnb_menu_link_click', 'user_engagement', 'page_view', 'view_item_list', 'general_link_click', 'page_view', 'view_item_list', 'proactive_message_impression', 'user_engagement']\n",
            "Generated Sequence: ['first_visit', 'session_start', 'at_visibility', 'page_view', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'page_view', 'page_view', 'page_view']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sequence = df['Sequence of events'][446][:8]  # Original sequence of length n-3\n",
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Original Sequence:\",df['Sequence of events'][446])\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, prediction_steps=3, num_beams=5)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALJ0bWV9M6uu",
        "outputId": "4e361a13-ad45-4336-a973-7fc43adc4080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: ['first_visit', 'session_start', 'page_view', 'view_item', 'page_view', 'proactive_message_impression', 'session_start', 'view_item']\n",
            "Original Sequence: ['first_visit', 'session_start', 'page_view', 'view_item', 'page_view', 'proactive_message_impression', 'session_start', 'view_item', 'page_view', 'proactive_message_impression', 'view_item', 'page_view']\n",
            "Generated Sequence: ['first_visit', 'session_start', 'page_view', 'view_item', 'page_view', 'proactive_message_impression', 'session_start', 'view_item', 'product_review_visibility', 'product_review_visibility', 'product_review_visibility']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model, tokenizer, input_sequence, prediction_steps=3, top_k=50, temperature=1.0):\n",
        "    input_ids = torch.tensor([tokenizer.global_dict.convert_sequence_to_indices(input_sequence)]).to(device)\n",
        "    model.eval()\n",
        "    generated_sequence = input_sequence.copy()\n",
        "\n",
        "    for _ in range(prediction_steps):\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_length=input_ids.size(1) + 1,\n",
        "                do_sample=True,\n",
        "                top_k=top_k,\n",
        "                temperature=temperature,\n",
        "                pad_token_id=tokenizer.global_dict.event_to_index[\"PAD\"],\n",
        "                decoder_start_token_id=tokenizer.global_dict.event_to_index[\"START\"],\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Get the next predicted token (excluding the input sequence)\n",
        "        next_token_id = output_ids[0, input_ids.size(1):].item()\n",
        "        next_token = tokenizer._convert_id_to_token(next_token_id)\n",
        "\n",
        "        # Append the predicted token to the generated sequence\n",
        "        generated_sequence.append(next_token)\n",
        "\n",
        "        # Update the input_ids for the next prediction\n",
        "        input_ids = torch.tensor([tokenizer.global_dict.convert_sequence_to_indices(generated_sequence)]).to(device)\n",
        "\n",
        "    return generated_sequence"
      ],
      "metadata": {
        "id": "QMempYJeOB6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sequence = df['Sequence of events'][446][:8]  # Original sequence of length n-3\n",
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Original Sequence:\",df['Sequence of events'][446])\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, prediction_steps=3, top_k=50, temperature=1.0)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMUVu6MlOFXc",
        "outputId": "1ef33471-c821-4d68-f66d-c24fb3a73df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: ['first_visit', 'session_start', 'page_view', 'view_item', 'page_view', 'proactive_message_impression', 'session_start', 'view_item']\n",
            "Original Sequence: ['first_visit', 'session_start', 'page_view', 'view_item', 'page_view', 'proactive_message_impression', 'session_start', 'view_item', 'page_view', 'proactive_message_impression', 'view_item', 'page_view']\n",
            "Generated Sequence: ['first_visit', 'session_start', 'page_view', 'view_item', 'page_view', 'proactive_message_impression', 'session_start', 'view_item', 'product_review_visibility', 'product_review_visibility', 'session_start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sequence = df['Sequence of events'][454][:8]  # Original sequence of length n-3\n",
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Original Sequence:\",df['Sequence of events'][454])\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, prediction_steps=3, top_k=50, temperature=1.0)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PqIG8TDObCA",
        "outputId": "5266d81b-02e6-48ba-b2d4-e1737a619911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: ['first_visit', 'session_start', 'at_visibility', 'general_tab_click', 'general_tab_click', 'open_chat', 'general_tab_click', 'general_tab_click']\n",
            "Original Sequence: ['first_visit', 'session_start', 'at_visibility', 'general_tab_click', 'general_tab_click', 'open_chat', 'general_tab_click', 'general_tab_click', 'general_tab_click', 'page_view', 'user_engagement']\n",
            "Generated Sequence: ['first_visit', 'session_start', 'at_visibility', 'general_tab_click', 'general_tab_click', 'open_chat', 'general_tab_click', 'general_tab_click', 'open_chat', 'open_chat', 'open_chat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sequence = df['Sequence of events'][467][:8]  # Original sequence of length n-3\n",
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Original Sequence:\",df['Sequence of events'][467][:11])\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, prediction_steps=3, top_k=50, temperature=1.0)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dAy7rwLOfgW",
        "outputId": "e9b56cda-b9ba-411c-8bd6-fb2994d94a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: ['first_visit', 'session_start', 'at_visibility', 'page_view', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click']\n",
            "Original Sequence: ['first_visit', 'session_start', 'at_visibility', 'page_view', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click']\n",
            "Generated Sequence: ['first_visit', 'session_start', 'at_visibility', 'page_view', 'general_button_click', 'general_button_click', 'general_button_click', 'general_button_click', 'page_view', 'page_view', 'page_view']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['Sequence of events'][467]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "WC8CHgVIHYF8",
        "outputId": "220053bf-d17f-4cb1-8770-509730b98ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c00d08cb2652>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sequence of events'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m467\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model, tokenizer, input_sequence, max_length=10, num_beams=5):\n",
        "    input_ids = torch.tensor([tokenizer.global_dict.convert_sequence_to_indices(input_sequence)]).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            pad_token_id=tokenizer.global_dict.event_to_index[\"PAD\"],\n",
        "            decoder_start_token_id=tokenizer.global_dict.event_to_index[\"START\"],\n",
        "            early_stopping=True\n",
        "        )\n",
        "    output_sequence = [tokenizer._convert_id_to_token(id.item()) for id in output_ids[0]]\n",
        "    return output_sequence"
      ],
      "metadata": {
        "id": "foL7qYh-bXhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = dataframe['Sequence of events'][5][:8]\n",
        "print(\"Original Input Sequence:\", input_sequence)\n",
        "output_sequence = generate_sequence(model, custom_tokenizer, input_sequence, max_length=10, num_beams=5)\n",
        "print(\"Generated Sequence:\", output_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQniVSFBbZR6",
        "outputId": "85fe4fe6-5e9d-44a6-e2f7-b78a41fb0587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Input Sequence: ['first_visit', 'session_start', 'open_chat', 'open_chat', 'open_chat', 'page_view', 'proactive_message_impression']\n",
            "Generated Sequence: ['START', 'session_start', 'move_to_whatsapp', 'open_chat', 'open_chat', 'open_chat', 'open_chat', 'open_chat', 'open_chat', 'open_chat']\n"
          ]
        }
      ]
    }
  ]
}